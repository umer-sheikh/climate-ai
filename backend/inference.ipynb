{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "class BGEEmbeddingFunction():\n",
    "    def __init__(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        self.device = device\n",
    "        self.model = SentenceTransformer(\"BAAI/bge-large-en\", device=device)\n",
    "    \n",
    "    def __call__(self, input: list[str]) -> list[list[float]]:\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        embeddings = self.model.encode(input, normalize_embeddings=True)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "def init_embedding_model():\n",
    "    return BGEEmbeddingFunction()\n",
    "\n",
    "embedding_function = init_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.config import Settings\n",
    "\n",
    "def init_chroma(embedding_function):\n",
    "    # Create a persistent client\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=\"./chroma_db\",\n",
    "        settings=Settings(\n",
    "            anonymized_telemetry=False,\n",
    "            allow_reset=True,\n",
    "            persist_directory=\"./chroma_db\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create or get collection\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=\"conversations\",\n",
    "        embedding_function=embedding_function,\n",
    "        metadata={\"description\": \"Conversation QA pairs\"}\n",
    "    )\n",
    "    \n",
    "    return collection\n",
    "\n",
    "collection = init_chroma(embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_example(collection, query_text, n_results=5):\n",
    "    results = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example query results:\n",
      "\"Question: Please explain the level of inclusion of the planning process.\\nAnswer: I'm sorry, but based on the given context, it is unclear how your question is related to the topic. Could you please give me more information or context about what you are referring to as \\\"the planning process\\\"? This will help me provide you with a more helpful and detailed answer.\"\n"
     ]
    }
   ],
   "source": [
    "query = \"Please explain the level of inclusion of the planning process.\"\n",
    "results = query_example(collection, query, 1)\n",
    "print(\"\\nExample query results:\")\n",
    "print(json.dumps(results['documents'][0][0], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
